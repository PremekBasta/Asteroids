\chapter*{Závěr}
V této práci se nám úspěšně podařilo vymyslet a naimplementovat abstrakce v podobě senzorů a akčních plánů do vytvořené vesmírné hry.
Tyto abstrakce umožnili zrychlení výpočtu her a také daly agentům nástroj jak se lépe v prostředí orientovat a jak na něj vhodně reagovat.
Dále jsme se seznámili s algoritmy genetického programování a hlubokého Q-učení.
Z cílů, které jsme si v úvodu stanovili, byli tedy kompletně splněny první tři.

V závěrečné části práce jsme zmíněné algoritmy aplikovali v našem herním prostředí a provedli s nimi sérii experimentů.
V rámci nich jsme nalezli agenty, kteří se naučili bránit a útočit dostatečně dobře na to, aby pro člověka bylo téměř nemožné je porazit.
Tedy cíl nalézt agenty, kteří budou naši hru hrát velmi dobře, byl také splněn.

Bohužel se nám nepodařilo nalézt agenty, kteří by byli dobří v pohybování se po herním prostoru a to ať už za záměrem úhybného manévru, nebo z důvodu získání lepší strategické pozice.
Jako důsledek se proto agenti chovají nepřirozeně staticky a pro člověka není příliš zábavné s nimi hrát.
Proto musíme konstatovat, že cíl nalézt agenty s pestrým chováním splněn zcela nebyl.

V případě pokračování na této práci by stálo za to zamyslet se nad dalšími možnými akčními plány, obzvláště nad těmi, které by zlepšovaly pohyb vesmírné lodi po herním prostoru.
Hra může být také do budoucna lehce rozšiřitelná o další implementace agentů. Pro přidání dalšího agenta stačí implementovat jednoduchou metodu, která dostává stav hry a vrací akci a agent může být jednoduše do hry přidán. 

Kromě rozšiřování akčních plánů a přidávání dalších agentů by také mohlo být zajímavé prozkoumat další algoritmy umělé inteligence a vyzkoušet v rámci nich podobné přístupy, které byly použity v našich experimentech.

\addcontentsline{toc}{chapter}{Závěr}
