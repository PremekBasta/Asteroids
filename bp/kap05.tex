\tolerance=1600
\chapter{Provedené experimenty}
V předchozí kapitole jsme se seznámili s algoritmy, které jsou aplikovatelné i v našem prostředí.
V této kapitole se pomocí různých experimentů pokusíme nalézt agenty, kteří se budou v jistém pojetí chovat inteligentně.

Abychom mohli výsledky experimentů vyhodnocovat, případně vzájemně mezi sebou porovnávat, potřebujeme mít na výsledky konkrétní kritéria.
Při testování obranného akčního plánu se ukázalo, že agent, který využívá pouze obranného akčního plánu k sestřelení nejbližšího asteroidu, se kterým vesmírné lodi hrozí srážka, dokázal díky dobré obraně zdatně přežívat ve hře mnoho kroků.
A zároveň je tento obranný agent vzhledem k využívání pouze jednoho akčního plánu dostatečně neinteligentní na to, abychom ho mohli použít jako referenčního agenta.

Jako kritérium pro hodnocení výsledků experimentů tedy bude sloužit souboj mezi výsledným agentem experimentu a obranným agentem.
První z nich, kterému se podaří zvítězit desetkrát, bude označen za vítěze. Výsledky těchto soubojů pak budou sloužit jako porovnání mezi jednotlivými provedenými experimenty.

Kromě primárního cíle nalézt agenty s inteligentním chováním máme i cíl sekundární, a to nalézt agenty, kteří se budou chovat zajímavě ve smyslu pestrosti akcí, akčních plánů, nebo také zajímavě v tom smyslu, že bude jejich chování působit jako chování lidského hráče.

\section{Genetické programování}
V prvních experimentech využijeme již zmíněného genetického programování. Nezbytným požadavkem pro jeho využití je existence reprezentace jedince a fitness funkce, která ho ohodnotí. 
Obojí dokážeme jednoduše vyřešit.
Jedinec bude představovat rozhodovací funkci, která se na základě vstupních argumentů rozhodne, který akční plán bude vybrán.

V našem případě máme hru, kde spolu dva hráči soupeří a hra končí výhrou jednoho z hráčů.
Přesně tohoto můžeme ve fitness funkci jedince využít. 
Pro využití výsledku hry, jak jedinec ve hře dopadl, musíme nejprve zvolit proti jakému hráči bude jedinec, který je předmětem našeho zájmu, hrát.


Pro experimentování s genetickým programováním jsem zvolil knihovnu deap pro python. Zde lze jednoduše konfigurovat evoluční algoritmus na konkrétní řešený problém.
Stačí popsat jak reprezentovat jedince a jak se vypočítá jeho fitness funkce a zbytek knihovna vyřeší za nás.

\subsubsection{Reprezentace jedince}

Ve 3. kapitole jsme si vybudovali abstrakce v podobě senzorů a akčních plánů a těch zde budeme chtít využít.
Jedince budeme podobně jako u symbolické regrese reprezentovat stromem.
Strom jedince budeme budovat prvky z následující množiny terminálů a neterminálů.

\begin{itemize}
\item{
    Terminály:
    \begin{itemize}
        \item Vstupní argumenty rozhodovací funkce (viz níže)
        \item Celočíselné konstanty -1, 1, 3, 5, 10, 100
        \item Nulární funkce vracející hodnoty reprezentující zvolený akční plán  
    \end{itemize}    
    }    
\end{itemize}
Jako argumenty funkce jsem zvolil následující hodnoty: délky všech čtyř akčních plánů a počet kroků před srážkou vesmírné lodi s asteroidem.
    Délky akčních plánů se pohybují v intervalu $(1,100)$, proto jsou číselné konstanty zvoleny tak, aby se jejich sčítáním a násobením lehce dosáhlo dalších hodnot z tohoto intervalu.


\begin{itemize}

\item{
    Neterminály:
    \begin{itemize}
        \item Aritmetické operace sčítání a násobení
        \item Funkce \emph{compare}
        \item Funkce \emph{if\_then\_else}
    \end{itemize}
}
\end{itemize}
Z aritmetických operací nám stačí sčítání a násobení. Operaci odčítání získáme pomocí sčítání a násobení konstantou -1. 
Hodnoty z intervalu $(1,100)$ jednoduše získáme také pomocí sčítáním a násobením potřebných konstant, proto pro operaci dělení není důvod.
Všechny aritmetické operace jsou typu $(int, int) \rightarrow int$
Funkce \emph{compare} je typu $(int, int) \rightarrow bool$, ta vrací zda je první argument větší než druhý.
Poslední použitá funkce \emph{if\_then\_else} je typu 
\newline
$(bool, ActionPlanEnum, ActionPlanEnum)\rightarrow ActionPlanEnum$. 
Tato funkce dostává jako argumenty výraz typu bool a následně dvě hodnoty reprezentující akční plány. 
Na základě pravdivosti výrazu vrací funkce první nebo druhou z hodnot akčních plánů.

Takto popsaná reprezentace jedince bude použita ve všech následujících experimentech. To, v čem se budou experimenty lišit, je způsob výpočtu fitness funkce a průběh evolučního algoritmu.

\subsection{Experiment 1: Soupeření s obranným agentem}
Cílem tohoto experimentu bylo vyvinout agenta, který bude lepší než obranný agent.
Hra je pokaždé velmi náhodná, tedy zahrání jedné hry by mělo nízkou vypovídající hodnotu. Proto jsem pro přesnější informaci zvolil zahrání šesti her.
Hodnota zahrané hry se skládá z více částí.
\begin{itemize}
    \item Počet kroků trvání hry
        \newline
        Myšlenkou je zde, obzvláště v počátku evoluce, upřednostňovat takové jedince, kteří dokáží vydržet ve hře co nejdéle, tedy nejsou ve hře okamžitě poraženi.
        Pro představu, délky her se pohybují přibližně v intervalu $(900, 2900)$ kroků.
    \item Penalizace za nevyužití některého z plánů
        \newline
        Během hry se udržuje historie, kolikrát se agent rozhodl pro každý z akčních plánů.
        Za každý ze čtyř akčních plánů, který agent ani jednou během hry nezvolil bude přičtena penalizace -500. Cílem těchto penalizací je upřednostňovat takové jedince, kteří používají všechny akční plány a tím pádem mají pestřejší chování. 
    \item Bonus (penalizace) za výhru (prohru)
        \newline
        Toto je asi nejdůležitější část. Pro zdůraznění rozdílu mezi vyhranými a prohranými hrami se v případě výhry přičte k výsledku hodnota 2000 a v případě prohry se 2000 odečte.
        Motivací mohou být následující dvě situace. Řekněme, že v jedné hře se podařilo jedinci dlouho bránit a dokázal vydržet 2500 kroků hry a poté prohrál. A v další situaci jedinec porazil soupeře v rychlých 1200 krocích. 
        Bez bonusu za vyhranou hru, by prohraná hra získala jedinci daleko vyšší hodnotu, než situace z druhé hry, kterou vyhrál.        
    
\end{itemize}

Algoritmus byl spuštěn s následujícími parametry:
\begin{itemize}
    \item Velikost populace: 30
    \item Pravděpodobnost křížení: 60\%
    \item Pravděpodobnost mutace: 20\%
    \item Genetické operátory: křížení dvou rodičů, jednobodová mutace a mutace celého podstromu
    \item Počet generací: 100
    \item Metoda selekce: turnajová selekce
\end{itemize}

Výsledný nejlepší jedinec bohužel nesplnil naše očekávání a nedokázal obranného agenta porazit. 
V souboji jedinec nejen nedokázal konkurovat obrannému agentovi, ale ani se neřídil příliš pestrou strategií.

V 95\% volil, stejně jako obranný agent, obranný akční plán a ve zbylých pár procentech volil všechny zbylé akční plány (viz \ref{Výsledek experimentu 01}).
Vyžívání všech akčních plánů bylo pravděpodobně dosaženo právě skrze vysokou penalizaci při nepoužití libovolného z nich, ale vidíme, že agent je volil spíše právě z tohoto důvodu, než že by je chtěl aktivně využívat v rámci své strategie.

\begin{figure}[H]\centering
\includegraphics[width=125mm, height=100mm]{./Obrazky/Experiment01Results.png}
\caption{Výsledek experimentu 1}
\label{Výsledek experimentu 01}
\end{figure}


\subsection{Experiment 2: Postupné zaměňování úspěšnějšího jedince}
V tomto experimentu nebylo cílem porazit konkrétního, stálého agenta jako v předchozím případě.
Zde bylo cílem postupně vybudovat nejzdatnějšího jedince.
Stejně jako v předchozím experimentu i zde fitness funkce spočívá v zahrání šesti her,
avšak zde nebudeme hrám přiřazovat žádnou číselnou hodnotu, ale spokojíme se s jednoduchou informací, který z agentů v dané hře zvítězil.
\par
V průběhu evoluce si budeme pamatovat současného nejlepšího jedince. 
Na začátku bude tento jedinec vybrán zcela náhodně. Obvyklým způsobem vytvoříme počáteční populaci a započneme evoluci.
Fitness funkce jedince bude počítat poměr, kolik ze šesti zahraných her jedinec vyhrál v souboji se současně nejlepším nalezeným jedincem.
Evoluce hledá řešení, která budou proti současnému nejlepšímu jedinci co nejúspěšnější.
V každé 3. generaci se následně kontroluje, zda již náhodou nebyl v populaci nalezen jedinec, který, pro současnou situaci, nejlepšího jedince porazil alespoň v pěti ze šesti her.
Pokud ano, tak takový jedinec bude nově zvolen jako nejlepší a evoluce bude pokračovat stejným způsobem dál.
\par
Po výměně nejlepšího jedince musíme nově přepočítat fitness funkci všech stávajících jedinců v populaci, protože jejich současná hodnota se vztahovala k původnímu soupeři.
Rovněž musíme, ze stejného důvodu, smazat všechny jedince ze síně slávy (ang. Hall of fame), kde se průběžně ukládají nejlepší jedinci spolu s hodnotou jejich fitness funkce.

\par
Všechny tyto změny už nelze v knihovně deap nakonfigurovat přímočarým způsobem jako v předchozím experimentu, ale bylo zapotřebí upravit samotnou kostru evolučního algoritmu.

\par
Algoritmus byl spuštěn s následujícími parametry
\begin{itemize}
    \item Velikost populace: 10
    \item Pravděpodobnost křížení: 60\%
    \item Pravděpodobnost mutace: 20\%
    \item Genetické operátory: křížení dvou rodičů, jednobodová mutace a mutace celého podstromu
    \item Počet generací: 450
    \item Metoda selekce: turnajová selekce
\end{itemize}

Výsledného agenta jsme nechali zahrát souboj s obranným agentem a tentokrát přinesl experiment daleko lepší výsledky.
Nalezený agent se oproti předchozímu experimentu dokázal naučit lépe útočit, volil útočný akční plán téměř ve 40\% případech a díky tomu dosáhl našeho primárního cíle. V Souboji porazil obranného agenta se skóre 10:0 a tím splnil primární cíl porazit obranného agenta.
Nicméně ani tentokrát se agent nenaučil nic jiného než obranu a útok (viz \ref{Výsledek experimentu 02}) a proto agentovo chování opět není příliš pestré. 

 


\begin{figure}[H]\centering
\includegraphics[width=125mm, height=100mm]{./Obrazky/Experiment02Results.png}
\caption{Výsledek experimentu 2}
\label{Výsledek experimentu 02}
\end{figure}



\newpage
\subsection{Experiment 3: Postupné zaměňování úspěšnějšího jedince bez obranného akčního plánu}

V předchozích experimentech se nám v obou případech podařilo vytvořit agenty, kteří v drtivé většině stavů rozhodují jen mezi obranným a útočným plánem.
To má za následek, že se agenti po celou dobu hry pouze otáčejí a střílejí, ale zůstavají při tom na jednom stejném místě.
V tomto experimentu tomuto problému zkusíme předejít, tím, že donutíme agenta bránit se uhýbáním namísto sestřelování nebezpečných asteroidů.

\par
\tolerance=10000
Experiment probíhá stejným způsobem jako v předchozím případě, jen s tím rozdílem, že agentovi zakážeme používání obranného plánu. 
Z argumentů rozhodovací funkce odstraníme informaci o obranném plánu. 
A z množiny terminálů používaných při tvorbě programů odstraníme nulární funkci reprezentující obranný akční plán.

\par
\tolerance=1600
To je vše co je potřeba změnit a zbylá logika může zůstat stejná jako v předešlém experimentu.

Algoritmus byl spuštěn s následujícími parametry
\begin{itemize}
    \item Velikost populace: 10
    \item Pravděpodobnost křížení: 60\%
    \item Pravděpodobnost mutace: 20\%
    \item Genetické operátory: křížení dvou rodičů, jednobodová mutace a mutace celého podstromu
    \item Počet generací: 2000
    \item Metoda selekce: turnajová selekce
\end{itemize}

S výsledným agentem jsme opět provedli souboj s obranným agentem.
První čeho si můžeme všimnout je, že náš agent v souboji prohrál s výsledkem 2:10, tedy bez obranného akčního plánu nebyl schopný tak úspěšně konkurovat obrannému agentovi.
Druhá skutečnost, která stojí za povšimnutí je průměrná délka hry, ta byla v průměru přibližně o 500 kroků kratší než v předchozím experimentu. 
Z toho vyplývá, že využívání úhybného akčního plánu k přežívání není tak účinné, jako bránění se pomocí obranného akčního plánu. 
To ale není nijak překvapivé. Pro agenta je prostředí tím víc nebezepečné, čím více je v něm nebezpečných asteroidů. 
Používání úhybného akčního plánu vede k vyhnutí vesmírné lodi před nebezepčným asteroidem, ne před jeho zničením, jako je to u obranného akčního plánu. To má za následek, že v případě úhybného akčního plánu agent neredukuje počet nebezepčných asteroidů a mnohem dříve se dostane do stavu, kdy je pro agenta příliš obtížné se roji asteroidů vyhnout. 

\par
Zajímavým výsledkem experimentu je také to, že, přestože agent používá v rámci úhybného akčního plánu akceleraci pro obranu velmi často, ani tentokrát nepoužívá zastavovací akční plán (viz \ref{Výsledek experimentu 03}).
To ukazuje, že zastavování letu není pro získání lepších výsledků stěžejní.
\par
Výsledný agent v souboji jednoznačně dosáhl špatných výsledků, ale pokud se na hru podíváme z lidského pohledu, tak oproti agentům, kteří zůstávají po celou dobu hry na místě, působí agentovo chování mnohem zajímavěji.


\begin{figure}[H]\centering
\includegraphics[width=125mm, height=100mm]{./Obrazky/Experiment03Results.png}
\caption{Výsledek experimentu 3}
\label{Výsledek experimentu 03}
\end{figure}




\newpage
\section{Hluboké Q-učení}
Herní prostředí nám v každém kroku vrací odměnu, kterou oba z hráčů za svou akci obdrželi. Tuto informaci jsme v experimentech provedených v rámci genetického programování nevyužili, ale zde bude mít zásadní roli.
Za každý krok, kdy hra ještě neskončila, získávají agenti automaticky odměnu 1. Na konci hry agent obdrží vysokou odměnu 2000 v případě výhry a v případě prohry naopak získá penalizaci v podobě odměny vysoké záporné hodnoty -1000.
Odměnu za výhru, nebo prohru získá agent až na úplném konci hry, to může ztěžovat učící proces. Proto prostředí dává agentům i průběžné menší odměny, pro lepší možnost učení se.

Konkrétně to jsou následující odměny:
\begin{itemize}
    \item Sestřelení asteroidu
        \newline
        Za každý sestřelený asteroid získává agent odměnu hodnoty 5.
    \item Zasažení nepřátelské vesmírné lodi asteroidem 
        \newline
        Zranění nepřítele je právě to, co agent potřebuje pro přiblížění se vítězství, proto za každé takové zasažení získává od prostředí odměnu v hodnotě 20.
    \item Zasažení vlastní vesmírné lodi asteroidem
        \newline
        Takový stav je pro agenta znevýhodňující a cílem je se mu vyvarovat, proto za takovýto stav agent od prostředí dostává penalizaci v hodnotě -10.
\end{itemize}


V rámci učení agentů budeme využívat $\epsilon$-hladového (ang. $\epsilon$-greedy) přístupu.
V každém kroku hry vygenerujeme náhodnou hodnotou z intervalu $(0,1)$ a pokud je tato hodnota menší než hodnota $\epsilon$, tak provedeme volbu akce náhodně, v opačném případě volíme nejlepší akce dle Q-sítě.
Hodnota $\epsilon$ se na počátku inicializuje na hodnotu 1 a po každé zahrané hře se sníží vynásobením koeficientem menším než 1. 
Průběžným snižování hodnoty $\epsilon$ způsobíme, že z počátku učení se budou zkoušet náhodné akce a v průběhu přejde z prohledávání nových akcí ke zkoušení již osvědčených akcí.


\par
Při trénování se nám stává, že měníme funkci, která odhaduje Q a tím je ovlivněno i chování agenta a odhady. K zachování větší stability trénování využijeme konceptu přehrávání zkušeností (ang. Experience replay) (\cite{experienceReplay}).
Při hraní hry si v každém kroku uložíme do paměti pětici současného stavu, provedené akce, obdržené odměny, stavu, do kterého jsme se dostali, a informace zda hra neskončila.
Po konci zahrání hry následně náhodně vybereme tyto pětice z paměti a trénování provedeme na nich.



\subsection{Experiment 4: Soupeření s obranným agentem}
V prvním experimentu jsme za pomocí genetického programování hledali agenta, který je úspěšný v souboji s obranným jedincem. Pro určení jak agent v souboji obstál jsme využívali fitness funkci.
Zde, pomocí hlubokého q-učení, budeme také učit agenta vzájemnými souboji s obranným agentem, ale budeme namísto fitness funkce pro trénování využívat odměny.

Společné s prvním experimentem zde bude také stejný přístup ke vstupům a výstupům. 
Na vstupu budou opět délky všech čtyř akčních plánů a počet kroků před srážkou vesmírné lodi s asteroidem a na výstupu čtyři hodnoty reprezentující výběr konkrétního akčního plánu.

Q-učení spočívá v učení se rozhodování akcí. Akce zde v tomto pojetí však nebudou představovat elementární akce, nýbrž akční plány. 
Q-síť bude tedy volit akční plány a proto zde budeme muset provádět mezikrok pro přechod od akčních plánů k akcím.
Nejprve vždy zvolíme akční plán a následně pro pokračování v simulaci hry z vybraného akčního plánu vybereme první akci.
V tomto experimentu budeme využívat přehrávání zkušeností, tj. budeme průběžně ukládat pětice informací o přechodech do dalších stavů. 
I zde pro pamatování si zkušenosti platí, že akcí budeme rozumět akční plán.


Parametry experimentu:
\begin{itemize}    
    \item Q-síť je hustá neuronová síť s pěti vstupy, čtyřmi výstupy a jednou skrytou vrstvou. 
    \item Během učení bude zahráno 1500 her.
    \item Konstanta pro snižování $\epsilon$ je nastavena na 0.998. To znamená, že například po zahrání 1400 her se bude v další hře volit akce náhodně jen v 6\% případů. 
\end{itemize}

\par
V souboji s obranným agentem se výslednému agentovi podařilo zvítězit pouze ve čtyřech hrách. 
Nepodařilo se nám tedy sice nalézt agenta, který by stabilně porážel obranného agenta, ale dosáhli jsme jiného zajímavého výsledku.
Velkým přínosem tohoto experimentu je pestrá strategie nalezeného agenta. Výsledný agent ve velkém zastoupení používá všechny akční plány (viz \ref{Výsledek experimentu 04}). 
Výsledkem je agent, který se brání nejen sestřelováním nepřátelských asteroidů, ale i vyhýbáním se, díky tomu se agent také pohybuje a nezůstává jen staticky stát na stejném místě po celou dobu hry.
Toho se nám také podařilo dosáhnout ve 3. experimentu, ale ve srovnání s agentem získaným ze 3. experimentu je tento agent daleko více obranyschopný.




\begin{figure}[H]\centering
\includegraphics[width=145mm, height=110mm]{./Obrazky/Experiment04Results.png}
\caption{Výsledek experimentu 4}
\label{Výsledek experimentu 04}
\end{figure}



\subsection{Experiment 5: Soupeření s obranným agentem - Rozšířeno}
V předchozím experimentu jsme dosáhli zajímavého chování agenta, ale nepodařilo se nám stabilně vyhrávat nad obranným agentem.
Zkusíme proto předchozí experiment rozšířit. 
V tomto experimentu zkusíme přidat další vstupní argumenty, které by mohli agentovi pomoct v rozhodování.

Přidané parametry:
\begin{itemize}
    \item Dvojice počtu zbývajících životů obou agentů            
    \item Počet nebezpečných asteroidů v blízké vzdálenosti od agenta
    \item Celkový počet nebezpečných asteroidů v celé hře
\end{itemize}
Snaha všech přidaných argumentů je rozšířit agentovi poznání o současném stavu hry a díky tomu mu dát možnost se komplexněji rozhodovat pro akční plány.

Parametry experimentu:
\begin{itemize}
    \item Q-síť je stejná síť jako v předchozím případě, jen namísto pěti vstupních argumentů, bude nyní přijímat vstupů devět.
    \item V tomto experimentu zkusíme kvůli rozšíření vstupních argumentů také prodloužit trénování sítě, proto bude v rámci trénování zahráno 3000 her.
    \item Adekvátně ke zvýšení počtu zahraných her také zvětšíme konstantu pro snižování $\epsilon$ z hodnoty 0.998 na 0.9989. Díky tomu bude stejné pravděpodobnosti 6\% pro volbu náhodné akce dosaženo přibližně po zahrání 2550 her.
\end{itemize}



Výsledný agent dopadl velmi úspěšně. Z průběhu trénování vidíme, že agent se velmi dobře učil a od přibližně 2300. hry (viz \ref{Průběh trénování experimentu 06}) už začal vyhrávat ve větší části her.
Rozšířením vstupních argumentů a přidání trénovacích her se nám podařilo zlepšit výsledek z předchozícho experimentu.
Agent sice ztratil pestrost akčních plánů, ale za to se významně zlepšil ve vyhrávání. Z výsledku 4:10 z předchozícho experimentu se zlepšil na 10:3.
Zajimavá na nalezeném agentovi je také jeho agresivita. Agent používá útočný akční plán přibližně dvakrát tak často jako obranný plán.


\begin{figure}[H]\centering
    \includegraphics[width=145mm, height=110mm]{./Obrazky/Experiment05Results.png}
    \caption{Výsledek experimentu 5}
    \label{Průběh trénování experimentu 05}
    \end{figure}



Při testování výsledného agenta jsem si všiml, že zahrání jedné hry je časově značně náročné, přičemž to co při simulaci trvalo netriviální objem času bylo samotné dotazování Q-sítě na akční plán.
Při snaze tento problém vyřešit jsem zjistil, že v některých stavech hry jsou všechny akční plány prázdné. 
Toto může nastat v případě kdy agent stojí na místě, není ohrožený žádným asteroidem a zároveň nenalezl žádný asteroid, kterým by mohl přímo ohrozit nepřítele.
V takovém stavu nemá velký smysl rozhodovat o volbě konkrétního akčního plánu. Proto jsem nastavil, že v takových případech agent rozhodování provádět nebude.

Podobně jsem také vypozoroval, že během jedné hry často nastane situace, že právě jeden z akčních plánů je neprázdný. Překvapením pro mě bylo, že Q-síť někdy v takových případech volila jiný prázdný plán před tímto neprázdným.
Proto jsem nastavil vyjímku i pro tyto případy a v současnou chvíli platí, že když agent má k dispozici právě jeden neprázdný akční plán, tak ho volí automaticky bez dotazování se Q-sítě.
Těmito opatřeními bylo dosaženo lepší časové náročnosti hraní hry a také byl agent v souboji úspěšnější. 
Zlepšení agenta si vysvětluji právě tím, že volba jakéhokoliv neprázdného plánu před prázdným je vždy výhodnější.
Tím, že přepočítání akčních probíhá každých pár kroků, tak není pro agenta v příštím rozhodování problém tento plán opustit a začít následovat jiný.

\begin{figure}[H]\centering
    \includegraphics[width=145mm, height=110mm]{./Obrazky/Experiment05Training.png}
    \caption{Průběh trénování v experimentu 5 - Výsledné odměny jsou součtem počtu kroků hry a odměny (resp. penalizace) za výhru (resp. prohru). Samotné tyto odměny tvoří rozdíl v hodnotě 3000, díky tomu je z obrázku zřetelné, ve kterých hrách agent vyhrál.}
    \label{Průběh trénování experimentu 06}
    \end{figure}



\subsection{Experiment 6: Elementární agent proti obrannému agentovi}
V tomto experimentu zkusíme sestoupit od abstrakcí v podobě akčních plánů k elementárním akcím.
Tentokrát nebudeme Q-síť používat k volbě akčního plánu, ale přímo k volbě elementární akce.
Výsledný agent bude volit vždy jen jednu akci, proto nebudeme moci využít koncept přepočítávání akčních plánů a agent se bude muset rozhodovat v každém kroku.
Opět budeme k trénování využívat soubojů s obranným agentem a učit se na základě odměn získaných od herního prostředí.

\par
K pěti elementárním akcím (rotace vlevo, rotace vpravo, akcelerace, obyčejná střela a rozdvojovací střela), pro které se bude agent rozhodovat, přidáme navíc také možnost prázdné akce. 
Nebudeme zde volit akční plány, proto ani nemá dobrý smysl používat jejich délky jako argumenty pro rozhodování. Proto zde můžeme zvolit zcela jiný přístup.
Samotné simulace pro získání akčních plánů jsou výpočetně velmi náročné, a tedy díky tomu, že zde volíme jednodušší přístup, budeme schopni, oproti předchozím experimentům, zahrát v rámci trénování větší množství her.

\par
Jako vstupní argumenty jsem zvolil následující hodnoty:
\begin{itemize}
    \item Vektor současného pohybu vesmírné lodi
    \item Úhel natočení vesmírné lodi
    \item Počet uplynulých kroků od posledního výstřelu
    \item Relativní poloha nepřátelské lodi (vektor spojující střed vlastní lodi a střed nepřátelské lodi v její nejbližší možné poloze)
    \item Relativní polohy tří nejbližších nebezpečných asteroidů (tři vektory spojující střed vlastní vesmírné lodi se středy tří nejbližších nebezepčných asteroidů)
\end{itemize}

Parametry experimentu:
\begin{itemize}
    \item Q-síť je hustá neuronová síť s dvěmi skrytými vstvami, čtrnácti vstupními a šesti výstupními hodnotami.
    \item Díky nevyužívání akčních plánů bude hraní her rychlejší, proto pro trénování zahrajeme 10000 her.
    \item Konstanta pro snižování $\epsilon$ je nastavena na hodnotu 0.9997
\end{itemize}


Výsledný agent proti obrannému agentovi nedopadl úspěšně. V souboji byl jednoznačně poražen se skóre 0:10.
Z přehledu používaných akcí během souboje můžeme i vypozorovat proč takto dopadl. Z elementárních operací se rozhodoval v drtivé většině pro rotaci vlevo a rozdvojovací střelu.
To v praxi znamená, že se agent naučil točit dokola a kdykoliv může, tak vystřelit. Tato strategie skutečně přináší nějaké výsledky.
Touto kombinací rotace a střelby se agent dokáže ubránit před srážkou s některými asteroidy, které by ho jinak zasáhly. Zároveň tímto způsobem sestřeluje netriviální množství asteroidů, které se kolem něho nacházejí a tím potenciálně staví nepřítele do ohrožení.
Avšak pro toto chování se agent rozhoduje bezmyšlenkovitě. 
Nemíří na žádné konkrétní asteroidy, ani na nepřátelskou loď.
Největší slabinou je, že agent se zde prakticky vůbec nenaučil bránit.
Veškeré asteroidy, před kterými se agent ubrání, zasáhne díky náhodě.


\begin{figure}[H]\centering
\includegraphics[width=145mm, height=110mm]{./Obrazky/Experiment06Results.png}
\caption{Výsledek experimentu 6}
\label{Výsledek experimentu 06}
\end{figure}
    



\subsection{Experiment 7: Dva elementární agenti}
V tomto experimentu budeme podobně jako v předchozím experimentu také pracovat s agenty využívajícími pouze elementární akce.
Tentokrát ale nebudeme při trénování hrát hry proti obrannému agentovi, nýbrž proti dalšímu elementárnímu agentovi, který bude také zároveň trénován.
Budeme tedy provádět dvojí Q-učení simultánně. Cílem je zde dosáhnout vzájemného adaptivního učení, kde se každý z agentů snaží zlepšovat proti svému nepříteli a postupně tak oba agenty zlepšovat.
Agenti budou reprezentováni neuronovou sítí stejného formátu jako v předchozím experimentu.

\par


Parametry experimentu:
\begin{itemize}
    \item Každý agent bude reprezentován vlastní Q-sítí stejného formátu jako v předchozím experimentu.
    \item Tím, že pro souboj nebudeme používat obranného agenta, ale dalšího elementárního agenta, ušetříme čas na výpočtu obranného agenta. V rámci trénování tedy zahrajeme 20000 her.  
    \item Konstanta pro snižování $\epsilon$ je nastavena na hodnotu 0.9998
\end{itemize}


\begin{figure}[H]\centering
\includegraphics[width=145mm, height=110mm]{./Obrazky/Experiment07Results.png}
\caption{Výsledek experimentu 7}
\label{Výsledek experimentu 07}
\end{figure}


Výsledný souboj jsme výjimečně neprovedli proti obrannému agentovi, ale mezi vzniklými agenty mezi sebou.
Z přehledu souboje je vidět, že se agenti od předchozího experimentu nijak zásadně nezlepšili. Oba agenti se v drtivé většině stavů jen točí na jednu stranu.
Vidíme, že každý agent volí exklusivně pouze jednu stranu, na kterou se rotuje. 
Agent 1 se z přehledu souboje zdá být mírně pestřejší, kombinuje oba typy střel a navíc v téměř pětině stavů volil akceleraci.
Když jsem však vizuálně sledoval souboj agentů, tak žádný z agentů nejevil známky komplexnějšího chování. 


\subsection{Experiment 8: Souboj nejlepších nalezených agentů}
V předchozích exprimentech jsme pomocí algoritmů genetického programování a hlubokého Q-učení nalezli více různých agentů. 
V tomto experimentu již nebudeme hledat dalšího agenta, místo toho zkusíme v souboji porovnat dva agenty, reprezentující nejlepší dosažený výsledek z každého z použitých algoritmů.
A nalézt tak celkově nejlepšího agenta.
\par
Z experimentů provedených v rámci genetického programování byl v souboji s obranným agentem nejúspěšnějsí agent z druhého experimentu, ten dokázal obranného agenta porazit se skórem 10:0.
Z druhé série experimentů dopadl nejlépe agent získaný v pátém experimentu, ten také dokázal porazit obranného agenta, avšak s horším celkovým skóre 10:4.

\begin{figure}[H]\centering
\includegraphics[width=145mm, height=110mm]{./Obrazky/Experiment08Results.png}
\caption{Výsledek experimentu 8}
\label{Výsledek experimentu 08}
\end{figure}

Vybrané agenty jsme proti sobě opět nechali zápasit v souboji do deseti výher libovolného z nich.
Z výsledku souboje vidíme, že agent z druhého experimentu, který dokázal obranného agenta porazit v souboji bez jediné prohrané hry, zvítězil s velmi solidním skóre 10:2 i v tomto souboji.
Zajímavé je také, jak se agenti v tomto souboji chovali.
Agent z pátého experimentu měl v původním souboji s obranným agentem velmi agresivní přístup a volil útočný akční plán v přibližně 68\% případech.
V tomto souboji však vidíme, že podíl volby útočného plánu klesl na přibližně 54\%.
Zato agent z druhého experimentu zůstal ve svém chování naprosto konzistentní. V souboji s obranným agentem byl jeho poměr útoku k obraně 40:60 a v souboji s agentem pátého experimentu se tento poměr změnil o jediné procento na 41:59.
\par
Agenta z druhého experimentu tedy můžeme označit jako celkového nejlepšího nalezeného jedince.

\section{Zkušenost z hraní her se získanými agenty}
V předchozích experimentech jsme nalezli různé umělé agenty a ukázali jsme výsledky jejich soubojů s obranným agentem.
Nyní by bylo vhodné nahlédnout na jednotlivé agenty i z lidského subjektivního pohledu, zkusil jsem si proto se všemi zahrát pár her.
Při větší snaze se mi podařilo porazit agenty ze třetího, šestého a sedmého experimentu, tedy agenty z těch experimentů, ve kterých dopadli neúspěšně i proti obrannému agentovi.
Výjimkou byl agent z prvního experimentu, ten sice v souboji s obranným agentem prohrál, ale v zápase se mnou dokázal úspěšně vzdorovat pravděpodobně z důvodu vysokého podílu volby obranného plánu.
V zápasech se zbylými agenty, kteří dopadli v experimentech úspěšně, jsem téměř neměl šanci. 
Zde agenti byli velmi agresivní a zároveň dělali minimální počet chyb.
Z celkového pohledu mohu říct, že hraní proti všem agentům působilo velmi uměle.
Někteří agenti stojí staticky na místě a střídají útok s obranou, agenti z posledních dvou experimentů se chovají zcela chaoticky, většinu času se točí na místě a střílejí do všech stran.
Jediný agent, který se zajímavě pohyboval a choval se v jistém smyslu rozumně, byl agent ze třetího experimentu, ten ovšem nedosahuje vysoké kvality a velmi rychle ve hrách se mnou prohrál.
Měl jsem možnost si hru zahrát i s dalším člověkem a v porovnání s touto zkušenosti nebylo hraní proti umělým agentům příliš zábavné.


